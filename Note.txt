딥러닝 네트워크에서는 노드에 들어오는 값들에 대해 곧바로 다음 레이어로 전달하지 않고 주로 비선형 함수를 통과시킨 후 전달한다.
 이때 사용하는 함수를 활성화 함수(Activation Function) 이라 부른다.

공분산 : 2개의 확률 변수의 상관정도를 나타내는 값이다.

PCA : 단순히 압축한다. ? 알집을 압축하는 것 처럼 예를 들어 50개의 칼럼이 있다면 그것을 가장 적절한 개수로 압축한다. 
      그 적절한 개수는 전혀다른 새로운 칼럼? 말이 됨? 

---------------------파일명-----------------
              dnn     lstm    cnn
iris           76,     77,    78 아이리스 꽃  
diabets?       79,     80,    81 당뇨병
brest cancer?  82,     83,    84 유방암



pd.get_dummies()

넘파이,판다스 공부하기

엄청나게 잘 정제된 데이터일 수록 레이어와 노드의 갯수가 꼭 많을 필요는 없다 

softmax 의 총 값은 1로 나오고 그중 가장 높은값을 argmax로 선택

earlystopping의 단점 -> 이미 과적합이 지나고 있는 부분이므로 시점만 파악하고 수동으로 튜닝한다

모델 저장하는 방법 

load_weight 뭐야 이거 -> 해결

가중치를 저장할 수 있는방법 3가지 modelcheckpoint, model.save(), model.save_weight()

예제 10가지에 대한 3가지 모델중 가장 최적화된 모델의 model.save, model.save_weight, modelcheckpoint

비선형 활성화함수 relu, sigmoid

backpropagation : 오차 역전파 
Vanishing Gradient Problem : 기울기 소실 문제
optimaizer : 반복 최적화 알고리즘 

'''
반복횟수에 맞는 적당한 레이어의 갯수가 필요하다? 
가중치가 너무 작은 값이면 가중치가 충분히 변화하지 않기 때문에 최적에 도달하기위해 많은 반복횟수를 요구하게되고 
너무큰 가중치는
'''

확장자가 .npy 로 저장하는 방법
csv파일 불러서 .npy

넘파이에서는 헤더까지 불러오면 에러가 난다
csv파일을 불러오고 첫행과 첫열을 사용할지 말지 판단해야함 

csv파일을 받아서 numpy로 저장해서 모델링하고 model.save를 이용해서 

hite = hite.fillna(method='bfill') -> nan에 전에 있던 값을 넣는다
hite = hite.fillna(method='ffill') -> nan에 이후에 있는 값을 넣는다

hite.loc['2020-06-02', '고가': '거래량'] = ['10','20','30','40']

타임 스텝스를 바꾸면 2일치의 예측 값이 나온다? -> 질문 

헤더가 2줄이라면 header = 1로 해서 0~1까지 헤더라고 인식시켜줘면된다 

iloc는 헤더와 인덱스 인덱스 번호를 이용
loc는 헤더와 인덱스의 이름을 이용

nan값은 부동소수점으로 인식됨 

cnn은 특성을 짤라서 한다 -> 나중에 시계열을 cnn으로 모델링하기 
lstm

앙상블할때 삼성을 2개의 모델과 하이트 진로의 모델 1개를 넣으면 삼성 쪽으로 가중치의 비중이 많아진다?



sklearn LinearSVC에 대해 알아보자 

회기일때 r2와 그냥 스코어 같은지 보고
분류일때 스코어와 에큐러시가 같은지 확인
그래서? 어떨때 어떤 모델을 쓸것인가에 대해 생각해보자

레거시한 머신러닝을 이용하여 결측치와 이상치를 채워준다 

LinearLogisticRegressor는 분류모델링 할때 주로 사용하는 것이지 회기 모델링을 할때 사용하는 오류를 범하지 말자
model.score는 자동으로 적당한 지표를 골라 계산하고 알려준다
회기에서 RandomForestRegressor로 모델링할때 지표를 acc로 잡는 실수를 범하지 말자 

SVC와 LinearSVC와의 차이점을 알아보자

그리드 서치 p.389 -> 내가 넣은 조건을 빠짐없이 테스트 해본다 -> 좋지만은 않음 -> 랜덤 서치

자동으로 복수개의 내부 모형을 생성하고 이를 모두 실행시켜서 최적 파라미터를 찾아준다.

그리드 서치와 랜더마이즈드 서치의 파라미터를 확인하고 비교해보기 

*****책이나 인터넷에서 그날그날 배운내용 찾아보기*****

0608
머신러닝에서는 원핫인코딩을 하지않아도 상관없다 
kfold에서 cv(crossvalidation)값에 따라서 데이터를 n등분

y값에 대한 결측치를 채워주는 판다의 기능이 있음 

선형에 잘맞는 PCA ??
featureimportance

파이프라인 -> 완전한 오토 하이퍼 파라미터튜닝

제너레이터를 이용하면 이미지를 수치화 할 수 있음?

conv1D

전처리 자동화 까지 minmax,standard -> pipeline
전처리 친구는 파이프라인 

x값 groupby 하기

0610
decision tree
feature importance

트리구조의 장점은 전처리가 필요없다 
2가지 장점이 있음 

내가 depth를 4에서 3으로 했을때 테스트 값에 대한 스코어값 

아이디어 
구현기술
계획수립
일정
최종결과 예측

훈련 장려금 5/11 ~ 6/5 20일 일단위로 계산 하루 15000

r2가 0에 가까울수록 알수없는 외부 요인이 많이 반영 됬다.(노이즈(에러)가 많이 반영되었다.)
0에 가까운데 결과가 ㄱㅊ다면 식이 잘못 됬을 수 있다.

XGB는 nan값에 대해 자동보간 기능이 있음 

값이 많으면 노드를 많이주라... ㅋㅋ

딥러닝 크로스 발리데이션 사용하기 

loss 가 가장작을때 최적의  w 벚어나면 과적합 
도달하지 못한경우? underfitting?

경사하강법 loss 최소의 loss

옵티마이저는 loss를 최적화 하는것 
이유는? 최적의 웨이트
optimizer = 'rmsprop'
optimizer = 'gdx 였나 여러가지가 있다'

에포에서 훈련을 할때 

그리드 서치를 쓰면 피쳐 임포턴스를 사용할 수 없음 그리드 서치를
먼저 써서 최적의 파라미터를 구하고 그 파라미터를 넣어서 피쳐임포턴스를 구하고 
모델 셀렉트 프롬을 이용하여 그리드 돌려서 다시 찾기

생체광학 결측치가 있는 열을 날렸더니 앞부분 36개의 칼럼만 남았다.

L1 Regularization(정규화) L2 Regularization 둘다 과적합을 막기위해 사용됨
Norm은 벡터의 크기를 측정하는 방법(함수)이다. 두 벡터 사이의 거리를 측정하는 방법이기도 하다

activation 기울기를 계산하는 수식이라고 생각하면되는 것인가?

normalization을 하는 이유는 비선형유지하기 위한 방법중 하나로 입력레이어에서 입력한 값이
첫번째 히든레이어를 지나면서 입력의 분포가 변하게 되고 이는 비선형성을 유지하기 힘들게만든다
때문에 비선형성을 유지시켜주기위해 스케일링? 하는 과정을 normalization이라고 한다. 맞나?ㅋ

과적합 방지
데이터를 늘린다
정규화 한다
피처수를 늘린다

과적합 훈련때 사용하지 않은 데이터에 대한 평가 test
validation 훈련할때 머신이 검증하는 부분 ?? 이거 자꾸 ...

경사하강법의 모멘텀 

옵티마이저에서 러닝레이트 

padding 토큰화 0 

임베딩 : 단어별로 공통점이 있는 수치를 압축? 벡터화 한다.

순환 신경망 모델
문장을 단어들의 시퀀스로 간주하고 순환(LSTM) 레이어의 입력으로 구성한 모델입니다.
임베딩 레이어 다음에 LSTM 레이어가 오는 경우에는 임베딩 레이어에 input_length 인자를 따로 설정할 필요는 없습니다.
입력 문장의 길이에 따라 input_length가 자동으로 정해지고, 이것이 LSTM 레이어에는 timesteps으로 입력되기 때문입니다.
블록으로 표현한다면 예제에서는 문장의 길이가 200 단어이므로, LSTM 블록 200개가 이어져있다고 생각하면 됩니다.

tensorflow 2.x from keras.layers import Dense -> from tensorflow.keras.layers import Dense (패치완료)

src == source
dst = destination

rb -> read to binary type
rt -> read to text type

오토인코더 ===================================================================================================

비지도학습 target값이 없는거
PCA -> x만 가지고함 (오토인코더의 친척 ㅋㅋ)
모자이크 마스크 씌우는게 비지도학습 -> 새로운 이미지의 생성 
앞뒤가 똑같은 오토인코더 ㅋㅋㅋㅋ
좋은경험 특성추출
사진에 잉크를 뿌리고 오토인코더를 돌리면 특성이 얼굴이 나옴 뚜렸하지는 않지만 흐릿하게 특성을 추출해줌 여기서 업그레이드 된것이 GAN
특성만 추출되는건 알겠는데 특성이 뭔줄알고 추출을 한다는 건지 잘 모르겠음 

우선 입력을 작게 만듬 특성을 추출? -> 다시 복구????
이게 왜하는거지 특성추출? ???? ??? ??? ??? ???

이미지를 확대했을때 손실되는 부분을 PCA나 오토 인코더를 통한 특성추출로 어느정도의 손실을 방지 할 수 있다...
무조건 큰값이 중요하다는건 큰값이 중요하지 않은 데이터일때는 오히려 역효과 라는말인가 아니면 어떤값일수록 중요한지 정해서 할수 있는 걸까 
모든 데이터에서 큰값이 무조건 중요해서 그게 특성일거라는 보장이 있을까??

손실이 너무 높다고 해서 예측률이 낮아지는 것은 아닐수도있음 

오토인코더 사이의 압축을 보기 위해서 pca를 해본거임 저거는 오토인코더가 아니다

오토인코더의 경유 acc보다 loss가 더 중요함 
==============================================================================================================

Structured Query Language

일정한 패턴을 예측 가능한 조합들로 나타내는것 이것이 나무의 그림과 비슷하다 하여 의사결정나무
일반화 성능을 향상시키고 과대적합을 방지할 수 있는 방법으로 앙상블 모델이 나왔고 의사결정나무의 앙상블 모델이 랜덤 포레스트

랜덤 포레스트(영어: random forest)는 분류, 회귀 분석 등에 사용되는 앙상블 학습 방법의 일종으로,
훈련 과정에서 구성한 다수의 결정 트리로부터 부류(분류) 또는 평균 예측치(회귀 분석)를 출력함으로써 동작한다